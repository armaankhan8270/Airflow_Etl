
```markdown
ğŸ“° News ETL Pipeline using Apache Airflow & PostgreSQL

A production-grade, fully local ETL pipeline that extracts real-time tech news from NewsAPI, transforms the data, and loads it into a PostgreSQL database â€” all orchestrated using Apache Airflow.

> âŒ No cloud tools. âœ… 100% Open Source. ğŸ³ Dockerized. ğŸ› ï¸ Real-world setup.

---

 ğŸ“Œ Features

- â° **Daily ETL Scheduling** with Apache Airflow
- ğŸ§ª **Data Cleaning & Normalization** during transformation
- ğŸ—ƒï¸ **Structured Storage** into PostgreSQL
- ğŸ” **Retry logic & Logging** with Airflow DAGs
- ğŸ“¦ **Dockerized** for easy setup and consistent environments

---

 ğŸ§± Project Structure


```markdown

news\_etl\_pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ news\_etl\_dag.py         # Main DAG definition
â”‚
â”œâ”€â”€ extract.py                  # Extract data from NewsAPI
â”œâ”€â”€ transform.py                # Clean and normalize article data
â”œâ”€â”€ load.py                     # Load data into PostgreSQL
â”‚
â”œâ”€â”€ docker-compose.yml          # Runs Airflow + PostgreSQL + Scheduler
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # API keys and DB credentials
â””â”€â”€ README.md                   # You're here

````

---

# âš™ï¸ Setup Instructions

# 1. Clone the Repository

```bash
git clone https://github.com/armaankhan8270/news-etl-pipeline.git](https://github.com/armaankhan8270/Airflow_Etl
cd news-etl-pipeline
````

### 2. Add Environment Variables

Create a `.env` file in the root directory:

```env
NEWS_API_KEY=your_news_api_key
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=news_db
```

> ğŸ”‘ You can get a free NewsAPI key from [https://newsapi.org/](https://newsapi.org/)

### 3. Start the Services

```bash
docker-compose up --build
```

This will launch:

* Airflow Webserver on `http://localhost:8080`
* PostgreSQL on `localhost:5432`

### 4. Initialize Airflow

In a new terminal:

```bash
docker exec -it airflow_webserver airflow db init
docker exec -it airflow_webserver airflow users create \
  --username admin --firstname Armaan --lastname Khan \
  --role Admin --email your@email.com --password admin
```

### 5. Trigger the DAG

* Visit `http://localhost:8080`
* Login using the credentials above
* Enable and trigger `news_etl_dag`

---

## ğŸ“Š Pipeline Overview

### ğŸ”¹ Extract

Pulls top tech headlines from NewsAPI and saves raw JSON.

### ğŸ”¹ Transform

Cleans and normalizes article data (removes HTML tags, filters empty fields, formats dates).

### ğŸ”¹ Load

Pushes cleaned articles into a `tech_news` table in PostgreSQL.

---

## ğŸ“ˆ Future Enhancements

* [ ] Sentiment Analysis on Articles
* [ ] Streamlit Dashboard for News Visualization
* [ ] Switch to Airflow 2.8+ and production-ready metadata DB
* [ ] Test coverage and logging improvements

---

## ğŸ’¡ Why This Project?

This project was built to simulate real-world, production-level ETL pipelines without relying on cloud platforms. Ideal for aspiring data engineers, it demonstrates:

* âœ… Clean DAG structure
* âœ… Modular Python scripts
* âœ… Real API + Real Database + Real Orchestration

---

## ğŸ“£ Connect With Me

I'm Armaan Khan, a 22-year-old Data Engineer passionate about building robust data systems and sharing what I learn.

* ğŸŒ [Portfolio](https://armaanswiftserve.netlify.app/)
* ğŸ’¼ [LinkedIn](https://linkedin.com/in/armaankhan8270)
* ğŸ™ [GitHub](https://github.com/armaankhan8270)

Feel free to fork the project, give it a â­ if you found it helpful, or connect to discuss more about data engineering!

---

> â€œCode more, rely less. Master the building blocks before the cloud.â€ â˜ï¸âŒğŸ’»âœ…

```

---

